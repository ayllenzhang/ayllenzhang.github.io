{"meta":{"title":"Ayllen's Neverland","subtitle":"","description":"Thoughts of research, technology and life","author":"Ayllen Zhang","url":"https://ayllenzhang.github.io","root":"/"},"pages":[{"title":"Product Documentation","date":"2021-06-19T17:23:18.275Z","updated":"2019-07-24T11:24:53.000Z","comments":true,"path":"about/index.html","permalink":"https://ayllenzhang.github.io/about/index.html","excerpt":"","text":"@column-2{ @card{ 关于我 张安然是自己给自己取的名字，理想是安然无恙，没病没灾。 会写一点点程序，研究兴趣很广泛，现在在做表示学习。 旅行爱好者，去过国内所有省，海外去过日韩美泰。同时是户外/徒步星人，时常露营。 目前为止最喜欢的是北海道的雪景，希望能在这儿养老。 喜欢听歌，最近听得多的是 寂寞的恋人啊 - 莫文蔚 南方 - 达达乐队 } @card{ About Me Graduated from THU in 2018, majoring in Automation. Summer intern @ School of Public Health, Brown University (2017) Summer intern @ Department of CST, Nanjing University (2019) Travel &amp; outdoor enthusiast. Travelled throughout China. My favorite by far is the snow scenery in Hokkaido Beloved songs California Dreamin' - The Mamas &amp; The Papas Seven Years - Norah Jones } } @column-2{ @card{ 关于这个博客 写些有意思的事，关于科研，关于旅行，关于日常。 闲鱼一条，随缘写文。 } @card{ About this blog A place for me to write some posts about research, travel experiences and others. Due to the limited network environment in China, loading speed of images could be a problem. Sorry for that. } }"},{"title":"","date":"2021-06-19T18:29:23.608Z","updated":"2021-06-19T18:29:23.608Z","comments":false,"path":"tags/index.html","permalink":"https://ayllenzhang.github.io/tags/index.html","excerpt":"","text":""},{"title":"","date":"2021-06-19T18:28:50.921Z","updated":"2021-06-19T18:28:50.921Z","comments":false,"path":"categories/index.html","permalink":"https://ayllenzhang.github.io/categories/index.html","excerpt":"","text":""},{"title":"archives","date":"2021-06-19T17:23:18.276Z","updated":"2019-07-24T11:24:53.000Z","comments":true,"path":"archives/index.html","permalink":"https://ayllenzhang.github.io/archives/index.html","excerpt":"","text":""}],"posts":[{"title":"摇曳露营的巡礼之旅，富士山行记","slug":"fuji-mnt","date":"2019-08-17T18:06:10.000Z","updated":"2021-06-19T18:00:47.178Z","comments":true,"path":"2019/08/fuji-mnt/","link":"","permalink":"https://ayllenzhang.github.io/2019/08/fuji-mnt/","excerpt":"时隔数月的回忆篇。（2021年注: 文中 b 站视频已失效，之后重传）","text":"时隔数月的回忆篇。（2021年注: 文中 b 站视频已失效，之后重传） 日本之行的第 5 天，在新宿站换好了 JR PASS ，就准备前往富士山了。 最初的打算是乘坐大巴（JR 所属的 JR Bus 是可以用 PASS 的），但到了与新宿站一条马路之隔的汽车站才发现当日的票已经卖完了，看来下次想乘巴士还得预定。查询车次后，选了如下的路线： fuji_route.png 将大行李存在新宿，只留下有着大部分露营用具的登山包和装着食材的咩姐袋子就出发了。 on_azusa.jpg 列车周边的景物一开始还是很东京的 azusa_city.jpg 没过多久就变成了山区景色 azusa_country.jpg 快到甲府的时候，能看到富士山的小脑袋了 azusa_little_fuji.jpg 到甲府后，换乘了长这样的身延线。明明没什么人嘛，为什么大巴没票了... shenyan.jpg 富士山以肉眼可见的速度变大 big_fuji.jpg 到了富士宫站以后才发现 14:13 出发的 已经是当日最后一趟巴士了。虽然之前已经在网上了解到这边巴士线路很少但没想到少到了这个地步，怪不得大多数巡礼党都是选择自驾前来。 fujibus_timetable.png 在富士急行巴士上又拍了一段，果然富士山就是好看啊 然后在「朝霧グリーンパーク」这个站下了车。从巴士站到露营地还要走个两公里的样子，而且是朝着富士山的反方向走，导致我一路上都在怀疑自己是否走错了。 asagiri_to_camp.png 然后一边走一边看身后的富士山，哈哈 campsite_road.jpg 终于走到了！ camp_site_frontdoor.jpg front_door_anime.jpg 这次去的是 ふもとっぱら 这个营地，对应摇曳露营的第 2 ~ 3 话。官网上是可以预约营地和住宿的，但是我并没有日本手机号就没有预约。由于是四月去的，也还没到旺季，所以当天去当天办理也完全可以。办完手续会给一张地图，和动画中的一模一样。顺带一提官网是有这个地图的电子版的。 fummoto_map.png 这里就是营地的办公室了，交钱领地图都在这儿，也是和动画中的一毛一样 yewuguanli_real.jpg yewuguanli.png 自带帐篷也需要 2000 日元的使用费，相当于国内三线城市睡一晚了 2000yen.png 開放感すげえ... kaihou.png kaihou_real.jpg 当然人多的时候是这样的 renduo.jpg 挑了块平整无水坑的草坪就开始扎营啦 顶着大风搭好了帐篷。大风天别说单人扎营，上次在百花山三人扎一个都难。可算完工了。 wangong.jpg yoshi.png Solo camp 真是太棒了！ 不过一路过来加上搭帐篷已经是又累又饿，是时候开饭了 kaifan.jpg burner.png 果然最好完成的露营美味就是火锅了呀。 hotpot.png 吃好喝好之后，就准备绕着营地走两圈了。 Double 富士山！ twofuji.png two_mountain.jpg twofujisample.png 这边的红房子据说以前是存放谷物的，不过现在已经只是洗手间了 redhouse_real.jpg redhouse.png 红房子的侧面是一张大脸 dalian_real.jpg dalian_anime.png 林子里的动物雕像，嗷呜~ aowu_real.jpg aowu_anime.png aowu.png 上面这几个地方的位置关系大概这样，狮虎雕像和办公室则要在更后方 position.png 没转多久太阳也就快落山了，不过今天并没有晚霞，只是一般般的红色 pink.png pinkornot.jpg 逐渐天黑了，大家都点起了帐篷里的小灯 tonight.jpg 由于天黑以后实在是无聊，八点多就睡了，准备四点起来看日出。睡前拍了个帐篷。 shuiqm.jpg 刚醒的时候天还完全没亮 gangxing.jpg 慢慢地稍微亮了一点点 shaoliang.jpg 把 pocket 放着拍日出了，我在旁边玩手机坐等 由于三脚架坏了只能放在炉头的盒子上拍，所以下面有个橙色盖子 2333。 我自己也用手机拍了两张。 morning1.jpg morning2.jpg 太阳刚出的时候地面会有一层雾，挺漂亮的。 去确认了食堂，然后发现食堂内部在装修中，也就没得吃了。 shitang_real.jpg shitang.png 只能再来一锅火锅了。由于早上太冷+风大在帐篷外根本煮不熟只好在帐篷里完成了。 jungu.jpg 这次是菌菇锅底。 之后就去办离场手续了。路上看见了供游客用餐的简易木屋，屋前的大树很漂亮。 xiaocanting_real.jpg 里面大概是这样的： xiaocanting.png 这才注意到还有个小纪念品，动画里也提到了这两条小狗。 dog2.png 溜啦溜啦 sayonara.jpg 出发去另一个露营地了，第一集中的浩庵露营场。 刚下车就看见了抚子睡的地方 gangxiache.jpg shuijiao.png 然后正前方就是浩庵露营场的前台和商店所在建筑了 koan_real.jpg koan_anime.png shoufu.jpg 相对于麓露营场，这边的设施要完善多了，也有很多食材和露营用具可购买。凛还感叹过麓露营场 noshop.png 这边有不少的露营周边卖，甚至可以微信支付 XguvbILV5taqEGz.jpg 因为露营场只有露营的人才能进去，我就只是在路边转了转。 chaiho.jpg 商店提供木柴，但林子里也是可以自己捡柴的。 然后我买了两个芝麻凛的挂饰。 guashi.jpg 开了两个扭蛋刚好是双女主，算是欧了一把 ouhuang.jpg 景色真好啊，拍了张我的小登山包和大富士山 langan2.jpg langan.png 最后一站是甲斐常叶，摇曳露营设定中学校所在的地方。站前有条浅溪。 zhanqianhe.jpg 溪对面有药店，咖啡店和杂货铺，也有很多周边卖。 zhanqian_anime.jpg zhanqian.jpg 第八集出现过桥对侧的视角，我中午就是在下图右侧的店里吃了盒方便面打发了。 dvce.png 左边山上就是学校，不过现实中已经废校了，农村人口减少的问题日本也存在啊。 jiafeimount.jpg 废校了也不确定能否进去，便只是远眺了一些。甲斐常叶这个小站在动画中也出现过 xiaozhan.jpg xiaozhan_anime.png 车来了，也意味着告别这次短暂的巡礼之旅了。等露营二期出了再来多巡礼一些地方吧！ chelaile.jpg","categories":[{"name":"远方 / Travel","slug":"Travel","permalink":"https://ayllenzhang.github.io/categories/Travel/"}],"tags":[{"name":"Japan","slug":"Japan","permalink":"https://ayllenzhang.github.io/tags/Japan/"},{"name":"Fuji","slug":"Fuji","permalink":"https://ayllenzhang.github.io/tags/Fuji/"}]},{"title":"Notes about MCMC and Sampling","slug":"mcmc-notes","date":"2019-07-26T11:43:46.000Z","updated":"2021-06-19T17:54:14.888Z","comments":true,"path":"2019/07/mcmc-notes/","link":"","permalink":"https://ayllenzhang.github.io/2019/07/mcmc-notes/","excerpt":"It all starts with a question: How to sample from a given distribution?","text":"It all starts with a question: How to sample from a given distribution? The simplest method Let's say, how to sample from \\(Uniform(0, 1)\\)? Actually the random() function, which is a pseudo-random number generator, have already solved the problem for us. It can generate a sequence of numbers whose properties approximate the properties of sequences of random numbers obeying uniform distribution. Moreover, we can sample from normal distribution by utilizing Box-Muller transform. Box-Muller transform If random variables \\(U_1, U_2 \\overset{\\text{iid}}{\\sim} U(0,1)\\), and \\[\\begin{align} Z_1=\\sqrt{-2\\ln{U_1}}cos(2\\pi U_2) \\\\ Z_2=\\sqrt{-2\\ln{U_1}}sin(2\\pi U_2) \\end{align}\\] then \\(N_1, N_2 \\overset{\\text{iid}}{\\sim} \\mathcal{N}(0,1)\\) Some other common continuous distribution e.g. exponential distribution, gamma distribution, beta distribution and t-distribution could also be sampling with similar math transform. However, when sampling from more complicated distributions or high dimensional distributions, it's not so easy to find such a transform. Here, complex sampling method will be used, and the algorithms we're going to introduce, MCMC(Markov Chain Monte Carlo) and Gibbs sampling are two of them that are commonly used. Some other methods Before looking directly at our goals, I'd like to introduce some other sampling methods first. Rejection sampling Also called accept-reject method. Given a distribution with p.d.f. \\(p(x)\\). Suppose there is another simple distribution function \\(q(x)\\) from which we can sample easily. If there exist a constant \\(M&lt;\\inf\\) that for all \\(x\\in\\mathbb{R}\\), \\[p(x)\\leq Mq(x)\\] then we can sample from \\(p(x)\\) with the following method: for i = 1 to N: Generate u ~ U(0, 1) and x ~ q(x) if u &lt; p(x) / (M * q(x)): x is accepted as a sample i += 1 else: continue It can be easily proved that this algorithm works. Importance Sampling Say if we want to know the expectation of \\(f(x)\\), i.e. \\(\\operatorname{E}[f(x)]\\), in which the random variable \\(x\\) follows a distribution \\(p(x)\\). Also, consider another simple distribution function \\(q(x)\\) that can be easily sampled. Then \\[ \\hat{E}=\\int_x{f(x)p(x)dx}=\\int_x{f(x)\\frac{p(x)}{q(x)}q(x)dx}\\] Let \\(w(x)=\\frac{p(x)}{q(x)}\\), the last formula can be seen as \\(\\operatorname{E}[f(x)w(x)]\\) in which \\(x\\sim q(x)\\). As we can easily sample from \\(q(x)\\), \\[\\hat{E}=\\frac{1}{N}\\sum\\limits_{i=1}^N{f(x^{(i)})w(x^{(i)})}\\] in which \\(x^{(i)}\\) is the \\(i\\)-th sampled \\(x\\) from \\(q(x)\\). Here \\(w(x)\\) is called importance weight. Markov chain Monte Carlo MCMC methods comprise a class of algorithms for sampling from a probability distribution. bMarkov Process, Markov Chain &amp; Hidden Markov Model Markov process Markov process is a stochastic process that satisfies the Markov property, i.e. the conditional probability distribution of future states of the process depends only upon the present state. Markov chain Markov chain is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event. For a Markov chain with finite state space, the transition probability distribution can be represented by a matrix, called the transition matrix, with the \\((i, j)th\\) element of \\(P\\) equals to \\[ p_{ij}=\\Pr(X_{n+1}=j\\mid X_{n}=i) \\] A Markov chain is called time-homogeneous if the transition matrix \\(P\\) is the same after each step. If the Markov chain is irreducible and aperiodic, then there is a unique stationary distribution π. \\[ \\pi\\mathbf{P}=\\pi \\] There are three several important properties for those fully connected (namely for all \\((i,j)\\) in \\(P\\), \\(\\exists n\\in\\mathbb{N^+} \\text{ such that }P^n_{ij}\\neq0\\)), aperiodic, time-homogeneous Markov chain. \\[ \\begin{equation} \\lim_{n\\rightarrow \\infty}{P^n}= \\begin{bmatrix} \\pi\\\\\\pi\\\\\\vdots\\\\\\pi\\\\ \\end{bmatrix} = \\begin{bmatrix} \\pi(1)&amp;\\pi(2)&amp;\\cdots&amp;\\pi(j)&amp;\\cdots\\\\ \\pi(1)&amp;\\pi(2)&amp;\\cdots&amp;\\pi(j)&amp;\\cdots\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;\\pi(j)&amp;\\cdots\\\\ \\pi(1)&amp;\\pi(2)&amp;\\cdots&amp;\\pi(j)&amp;\\cdots\\\\ \\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots&amp;\\vdots\\\\ \\end{bmatrix} \\end{equation} \\] \\[ \\pi(j)=\\sum_{i=0}^{\\infty}\\pi(i)P_{ij} \\] \\(\\pi\\) is the only non-negative solution of equation \\(\\pi\\mathbf{P}=\\pi\\) Detailed balance condition A Markov chain is said to be reversible if there is a probability distribution \\(\\pi\\) over its states such that \\[ \\pi _{i}\\Pr(X_{n+1}=j\\mid X_{n}=i)=\\pi _{j}\\Pr(X_{n+1}=i\\mid X_{n}=j) \\] the detailed balance equation can be written as \\[ \\pi _{i}p_{ij}=\\pi _{j}p_{ji}\\, \\] which indicates that, for each pair of states \\((i, j)\\), the probability mass gained from each other should be equal to the probability mass they loss at the same time, when the stationary distribution \\(\\pi\\) is reached. Hidden Markov Model HMM is a statistical Markov model in which the system being modeled is assumed to be a Markov process with hidden states and visible output state for each hidden state. The following image describes a Markov chain. \\(x(t-1)\\), \\(x(t)\\) and \\(x(t+1)\\) denote the hidden states at the 3 moment, while \\(y(t-1)\\), \\(y(t)\\) and \\(y(t+1)\\) denote the corresponding observer states. HMM.png Metropolis–Hastings algorithm M-H algorithm is one of the commonly used random walk Monte Carlo methods. In fact, the name Monte Carlo is suggested to use by Metropolis. As said above, MCMC is to construct a Markov chain with stationary distribution \\(\\pi=p(x)\\), in which \\(p(x)\\) is the given distribution to be sampled. Consider that we already know the given distribution \\(p(x)\\) and its transition matrix \\(Q\\). Generally we'll have \\[ p(i)q(i,j)\\neq p(j)q(j,i) \\] We can introduce another coefficient \\(\\alpha(i,j)\\) that will hopefully solve the problem for us, namely \\[ \\begin{equation} p(i)q(i,j)\\alpha(i,j)=p(j)q(j,i)\\alpha(j,i) \\tag{*} \\end{equation} \\] Clearly we can let \\(\\alpha(i,j)=p(j)q(j,i)\\) to make the equation true. If we let \\(q&#39;(i,j)=q(i,j)\\alpha(i,j)\\), the original equation would become: \\[ p(i)q&#39;(i,j)=p(j)q&#39;(j,i) \\] Now we have successfully constructed a Markov chain with transition matrix \\(q&#39;\\) and given stationary distribution \\(p(x)\\). The Metropolis–Hastings algorithm is described below. Notice that \\(q(j|i) = q(i,j)\\). x_&#123;0&#125; = x0 # Initialization for i = 1 to N: Generate x* ~ q(x*|x_&#123;i-1&#125;) a = min( 1, ( p(x*) * q(x*|x_&#123;i-1&#125;) ) / ( p(x_&#123;i-1&#125;) * q(x_&#123;i-1&#125;|x*) ) ) Generate u ~ U(0, 1) if u &gt; a: x_&#123;i&#125; = x_&#123;i-1&#125; # Reject else: x_&#123;i&#125; = x* # Accept When \\(q(i, j)=q(j,i)\\) is satisfied, the algorithm is called Metropolis algorithm, in which a = min( 1, p(x*) / p(x_&#123;i-1&#125;) ) Gibbs Sampling Gibbs sampling is a special case of the Metropolis–Hastings algorithm. Given a multivariate distribution, it's better for us to use Gibbs sampling if it is simpler to sample from a conditional distribution than to marginalize by integrating over a joint distribution. Suppose we want to obtain \\(N\\) samples of \\(\\mathbf {X}=(x_{1},\\dots ,x_{n})\\) from a joint distribution \\({\\displaystyle p(x_{1},\\dots ,x_{n})}\\). Gibbs sampling can be described as follows: X_0 = X0 # Initialization for i = 1 to N: for j = 1 to n: sample x^i_j from the distribution: p(x^i_j|x^i_1,...,x^i_&#123;j-1&#125;,x^&#123;i-1&#125;_&#123;j+1&#125;,...x^&#123;i-1&#125;_n) Slice Sampling Suppose you want to sample some random variable \\(X\\) with distribution \\(f(x)\\). Slice sampling can be described as follows: x_0 = x0 # Initialization for i = 1 to N: Generate y_i from U(0, x_&#123;i-1&#125;) Generate x* from U(Domain of x) while f(x*) &lt; y generate x* from U(min(x_&#123;i-1&#125;, x*), max(x_&#123;i+1&#125;, x*)) x_i = x*","categories":[{"name":"科研 / Research","slug":"Research","permalink":"https://ayllenzhang.github.io/categories/Research/"}],"tags":[{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://ayllenzhang.github.io/tags/Machine-Learning/"},{"name":"notes","slug":"notes","permalink":"https://ayllenzhang.github.io/tags/notes/"},{"name":"MCMC","slug":"MCMC","permalink":"https://ayllenzhang.github.io/tags/MCMC/"}]},{"title":"Probability statistics notes","slug":"proba-note","date":"2019-07-24T11:31:19.000Z","updated":"2020-11-17T01:08:35.000Z","comments":true,"path":"2019/07/proba-note/","link":"","permalink":"https://ayllenzhang.github.io/2019/07/proba-note/","excerpt":"Some notes I took down last summer, when I was reviewing statistics and probability.","text":"Some notes I took down last summer, when I was reviewing statistics and probability. Basic Probability Definitions Event: A set of outcomes of an experiment Random variable: outcome of an experiment Bayes' Theorem For two random events A, B \\[ P(B|A)=\\frac{P(AB)}{P(A)}=\\frac{P(AB)}{P(B)}\\cdot\\frac{P(B)}{P(A)}=\\frac{P(B)}{P(A)}\\cdot P(A|B) \\] namely, \\[ P(B|A)P(A)=P(AB)=P(A|B)P(B) \\] Independent events Events A and B are independent events if the occurrence of one of them does not affect the probability of the occurrence of the other. That is, two events are independent if either \\[ P(B|A)=P(B) \\] or \\[ P(A|B)=P(A) \\] Meanwhile, (A', B), (A, B'), (A', B') are also independent events if A, B are independent events. PMF, CDF and PDF PMF means probability mass function. Suppose that \\(X: S → A (A \\subseteq R\\)) is a discrete random variable defined on a sample space \\(S\\). Then the probability mass function \\(f_X: A → [0, 1]\\) for \\(X\\) is defined as \\[ f_{X}(x)=\\Pr(X=x)=P(\\{s\\in S:X(s)=x\\}) \\] Hyper geometric distribution is one of those common discrete distributions. If we randomly select n items without replacement from a set of N items of which: m of the items are of type-1 and N − m of the items are of type-2 then the PMF of X, the discrete variable defined as the number of selected type-1 items, is called hyper-geometric distribution, which is: \\[ P(X=x)=f(x)=\\frac{\\binom mx\\binom {N\\,-\\,m}{n\\,-\\,x}}{\\binom Nn} \\] CDF, also called cumulative distribution function, is generally a function of a real-valued random variable \\(X\\) given by \\[ F_{X}(x)=\\operatorname {P} (X\\leq x) \\] The CDF of a continuous random variable \\(X\\) can be expressed as the integral of its probability density function \\(f_X\\) as follows: \\[ F_{X}(x)=\\int _{-\\infty }^{x}f_{X}(t)\\,dt \\] PDF, known as probability density function, is similar with PMF while it's defined for continuous random variable. PDF is defined as follows. \\[ \\Pr[a\\leq X\\leq b]=\\int _{a}^{b}f_{X}(x)\\,dx. \\] Hence, if \\(F_X\\) is the cumulative distribution function of continuous random variable \\(X\\), then: \\[ F_{X}(x)=\\int _{-\\infty }^{x}f_{X}(u)\\,du \\] namely, if \\(f_X\\) is continuous at \\(x\\), \\[ f_{X}(x)={\\frac {d}{dx}}F_{X}(x) \\] uniform distribution is a common continuous distribution, in which the continuous random variable X has average probability in \\([a, b]\\), denoted as \\(X\\sim U(a, b)\\). Its probability density function is \\[ \\]f(x)=~~~(axb) $$ Expectation and variance Expectation For discrete variable \\(X \\in S\\), expectation of \\(x\\) is defined as: \\[ E(X)=\\sum_{x\\in S}{x\\cdot p(X=x)} \\] e.g. the expectation of hyper-geometric distribution can be calculated as follow: \\[ \\begin{align}E(X) &amp;=\\sum_{x\\in S}x\\cdot\\frac{\\binom mx\\binom {N\\,-\\,m}{n\\,-\\,x}}{\\binom Nn} \\\\ &amp;=\\sum_{x\\in S}\\frac{\\frac{m!}{(x-1)!(m-x)!}\\binom {N\\,-\\,m}{n\\,-\\,x}}{\\binom Nn} \\\\ &amp;=\\sum_{x\\in S}\\frac{m\\cdot\\binom{m-1}{x-1}\\binom {N\\,-\\,m}{n\\,-\\,x}}{\\binom Nn} \\\\ &amp;=\\sum_{x\\in S}\\frac{m\\cdot\\binom{N-1}{n-1}}{\\binom Nn} \\\\ &amp;=\\frac{mn}{N} \\end{align} \\] For continuous variable \\(X\\in \\mathbb{R}\\), expectation is defined as: \\[ {E} [X]=\\int _{\\mathbb {R} }xf(x)\\,dx \\] The expectation operator is linear in the sense that \\[ E(aX+bY)=aE(X)+bE(Y) \\] Variance The variance of a random variable \\(X\\) is the expected value of the squared deviation from the mean of \\(X\\). Let \\(\\mu = E[X]\\), \\[ \\operatorname {Var}(X)=E\\left[(X-\\mu)^{2}\\right]\\ \\] Substitute with \\(\\mu = E[x]\\), we can have \\[ \\begin{align} \\operatorname{Var}(X)&amp;=E\\left[(X-E[X])^{2}\\right]\\\\ &amp;=E\\left[X^2-2XE[X]+E[X]^2\\right]\\\\ &amp;=E[X^2]-2E[X]E[X]+E[X]^2 \\\\ &amp;=E[X^2]-E[X]^2 \\end{align} \\] The variance of a sum of random variables and constants is given by \\[ \\operatorname {Var} (aX+bY+c)=a^{2}\\operatorname {Var} (X)+b^{2}\\operatorname {Var} (Y)+2ab\\,\\operatorname {Cov} (X,Y) \\] where \\(\\operatorname{Cov}(\\cdot,\\cdot)\\) is the covariance. Covariance The covariance between two jointly distributed real-valued random variables \\(X\\) and \\(Y\\) is defined and transformed as: \\[ \\begin{align} \\operatorname{Cov}(X, Y)&amp;=E[(X-E(X))(Y-E(Y)]\\\\ &amp;=E[XY-XE(Y)-YE(X)+E(X)E(Y)]\\\\ &amp;=E[XY]-E[X]E[Y] \\end{align} \\] Note that \\(\\operatorname{Var}(X) = \\operatorname{Cov}(X, X)\\). Meanwhile, for random variables X, Y in joint support S, suppose \\(f(x,y)\\) is the joint probability density function defined on \\((x,y)\\in S\\). For X and Y are discrete random variables, \\[ \\operatorname{Cov}(x,y)=\\sum_{(x,y)\\in S}(x-\\mu_x)(y-\\mu_y)f(x,y) \\] and for X and Y are continuous random variables, \\[ \\operatorname{Cov}(x,y)=\\iint_{(x,y)\\in S}(x-\\mu_x)(y-\\mu_y)f(x,y)dxdy \\] Correlation The correlation is defined as: \\[ \\rho_{xy}=\\operatorname{Corr}(X,Y)=\\frac{\\operatorname{Cov}(X,Y)}{\\sigma_x\\sigma_y}=\\frac{\\sigma_{xy}}{\\sigma_x\\sigma_y}\\in[-1,1] \\] When \\(\\operatorname{Corr}(X,Y)\\) is close to \\(\\pm 1\\), a strong linear relationship between \\(X\\) and \\(Y\\) is indicated. Sample expectation and variance Here we suppose \\(X_1\\), \\(X_2\\), \\(\\dots\\) , \\(X_n\\) are observations of a random sample of size n. Then \\(\\bar{X}=\\frac{1}{n}\\sum_{i=1}^{n}{X_i}\\) is the sample mean of the n observations, and \\(S^2=\\frac{1}{n-1}\\sum_{i=1}^{n}{(X_i-\\bar{X})^2}\\) is the sample variance of the n observations Proof: Here we assume the original expectation(mean) and variance of \\(X_i\\) to be \\(\\mu, \\sigma^2\\). Then we have: \\[ E(\\bar{X})=E(\\frac{1}{n}\\sum_{i=1}^{n}{X_i})=\\frac{1}{n}\\sum_{i=1}^{n}{E(X_i)}=\\frac{1}{n}\\sum_{i=1}^{n}{\\mu}=\\mu \\] \\[ \\begin{align} E(S^2)&amp;=E\\left[\\frac{1}{n-1}\\sum_{i=1}^{n}{(X_i-\\bar{X})^2}\\right]\\\\ &amp;=\\frac{1}{n-1}\\sum_{i=1}^{n}E\\left(X_i-\\frac{\\sum_{j=1}^{n}X_j}{n}\\right)^2\\\\ &amp;=\\frac{1}{n-1}\\sum_{i}E\\left[X_i^2-2\\frac{X_i\\sum_{j}X_j}{n}+\\frac{(\\sum_j{X_j})^2}{n^2}\\right]\\\\ &amp;=\\frac{1}{n-1}\\left\\{\\sum_i{E(X_i^2})-E\\left[\\frac{(\\sum_i{X_i})^2}{n}\\right]\\right\\}\\\\ &amp;=\\frac{1}{n-1}\\left[\\frac{n-1}{n}\\sum_i{E(X_i^2)}-\\frac{1}{n}\\sum_{i,j\\neq i}E(X_iX_j)\\right]\\\\ \\end{align} \\] Notice that \\(X_1, X_2, ..., X_n\\) are independent variables, which means \\(\\forall i,j \\in 1,\\dots,n, Cov(X_i, X_j)=0\\). Namely, \\(E(X_iX_j)=E(X_i)E(X_j)\\). So we have \\[ \\begin{align} E(S^2)&amp;=\\frac{1}{n-1}\\left[\\frac{n-1}{n}\\sum_i{E(X_i^2)}-\\frac{1}{n}\\sum_{i,j\\neq i}E(X_i)E(X_j)\\right]\\\\ &amp;=\\frac{1}{n-1}\\cdot\\frac{n-1}{n}\\sum_i{\\left[E(X_i^2)-E(X_i)^2\\right]}\\\\ &amp;=\\frac{1}{n}\\cdot n\\operatorname{Var}(X_i)\\\\ &amp;=\\sigma^2 \\end{align} \\] It can be proved that \\(S^2\\) and \\(\\bar{X}\\) are independent. In addition, if \\(X_1, X_2, \\dots, X_n\\) subject to normal distribution, then \\[ \\frac{(n-1)S^2}{\\sigma^2}=\\frac{\\sum_{i=1}^{n}{(X_i-\\bar{X})^2}}{\\sigma^2}\\sim \\chi^2(n-1) \\] Moment Generating Function Consider the Taylor expansion form of exponential function \\(e^{tx}\\) at \\(x=0\\), which is \\[ e^{tx} = 1+tx+\\frac{(tx)^2}{2!}+\\frac{(tx)^3}{3!}+\\cdots \\] If we replace x with random variable X, and consider the expected value of both sides, the result will be \\[ E(e^{tX})=1+tE(X)++\\frac{t^2E(X^2)}{2!}+\\frac{t^3E(X^3)}{3!}+\\cdots \\] Here, we define the moment generating function of a continuous random variable X, if it exists, to be \\[ M(t)=E(e^{tX})=\\int_{-\\infty}^{+\\infty}e^{tx}f(x)dx \\] for \\(-h&lt;x&lt;h\\). From the equations above, it's clearly that \\[ M^{(r)}{(0)}=E(X^r)+\\sum_{i\\geq 1} a_it^i \\] Meanwhile, let \\(S\\) be the set of possible values of \\(X\\), then \\[ M(t)=E(e^{tX})=\\sum_{x\\in S}e^{tx}f(x) \\] Therefore, the coefficient of \\(e^{tx}\\) is the probability: \\[ f(x)=P(X=x) \\] Typical discrete distributions Binomial Distributions \\(2\\) results, \\(n\\) samplings. Let \\(X\\) to be the frequency of the result with probability \\(P\\) to be selected in a single sampling. Then we say X subjects to binomial distribution, write as \\(X\\sim B(n,p)\\). And we have: \\[ \\begin{align} P(X=k)&amp;={n \\choose k}p^{k}(1-p)^{n-k}\\\\ E[X]&amp;=np\\\\ \\operatorname{Var}(X)&amp;=np(1-p) \\end{align} \\] Poisson Distributions A Poisson distribution tries to describe such a situation: an event can occur 0, 1, 2, … times in an interval. The average number of events in an interval is designated \\(\\lambda\\). Lambda is the event rate, also called the rate parameter. Let random variable \\(X\\) to note the number of observed events in an interval. Then we have \\[ P(X=k)=e^{-\\lambda }{\\frac {\\lambda ^{k}}{k!}} \\] The mean and variance for Poisson distribution are both \\(\\lambda\\). Besides, notice that \\(\\sum_{i}\\lambda^i/i!\\) is the Taylor series of \\(e^x\\) at \\(x_0=0, x=\\lambda\\). Therefore, we can say definitely that \\[ \\sum_{k=0}^{+\\infty}P(X=k)=1 \\] In fact, Poisson distribution could be derived from binomial distribution. Suppose \\(n\\rightarrow +\\infty\\) while \\(np = const\\). Let \\(np = \\lambda\\), then \\[ \\begin{align} P(X=k)&amp;=\\lim_{n\\rightarrow +\\infty}{n\\choose k}p^k(1-p)^{n-k}\\\\ &amp;=\\lim_{n\\rightarrow +\\infty}\\frac{n(n-1)\\cdots(n-k+1)}{k!}\\cdot\\frac{\\lambda^k}{n^k}(1-\\frac{\\lambda}{n})^{n-k}\\\\ &amp;=e^{-\\lambda}\\lim_{n\\rightarrow +\\infty}(1-\\frac{1}{n})(1-\\frac{2}{n})\\cdots(1-\\frac{k-1}{n})\\frac{\\lambda^k}{k!}\\\\ &amp;=e^{-\\lambda}\\frac{\\lambda^k}{k!} \\end{align} \\] With \\(n\\geq 20, p\\leq 0.05\\), the Poisson distribution can be used to approximate the binomial distribution by setting \\(\\lambda=np\\). Typical continuous distributions Uniform Distributions We have introduced this kind of distributions above, here are some other properties of it. For \\(a\\leq x\\leq b\\), we have \\[ \\begin{align} F(x)&amp;=\\frac{x-a}{b-a}\\\\ \\mu=E(X)&amp;=\\frac{a+b}{2}\\\\ \\sigma^2=\\operatorname{Var}(X)&amp;=\\frac{(b-a)^2}{12} \\end{align} \\] Exponential Distribution The continuous random variable X follows an exponential distribution if its probability density function is: \\[ f(x)=\\frac{1}{\\theta}e^{-x/\\theta}~~~(x\\geq0,\\theta&gt;0) \\] For \\(x\\geq0,\\theta&gt;0\\). Besides, \\[ \\begin{align} F(x)&amp;=-e^{-x/\\theta}+1\\\\ \\mu=E(X)&amp;=\\theta\\\\ \\sigma^2=\\operatorname{Var}(X)&amp;=\\theta^2 \\end{align} \\] Here's an derivation of exponential distribution. Suppose a random event \\(e\\) happens randomly in \\(t\\in [0,\\infty)\\), with an average frequency \\(\\lambda\\) to happen in an interval of length 1. This is to say that the number of \\(e\\) in an interval follows Poisson distribution. Let \\(X\\) note the distribution of the first appearance of \\(e\\). Then we have \\[ \\begin{align} F(X=x)&amp;=P(X\\leq x)=1-P&amp;(e~happens~0~time~in~[0,x])\\\\ &amp;=1-e^{-\\lambda x}\\frac{(\\lambda x)^0}{0!}~~~&amp;\\text{(from Poisson distribution)}\\\\ \\Rightarrow f(x)&amp;=F&#39;(x)=\\lambda e^{-\\lambda x} \\end{align} \\] Let \\(\\theta = \\frac{1}{\\lambda}\\), then \\[ f(x)=\\frac{1}{\\theta}e^{-x/\\theta} \\] Gamma Distribution Derivation of Gamma Distribution Consider the derivation of exponential distribution. In the same way, we suppose a random event \\(e\\) happens randomly in \\(t\\in [0,\\infty)\\), with an average frequency \\(\\lambda\\) to happen in an interval of length 1. However, we let \\(X\\) note the distribution of the \\(\\alpha^{th}\\) appearance of \\(e\\). Then: \\[ \\begin{align} F(X=x)&amp;=P(X\\leq x)=1-\\sum_{i=0}^{\\alpha-1}P(e~happens~i~time~in~[0,x])\\\\ &amp;=1-\\sum_{i=0}^{\\alpha-1}e^{-\\lambda x}\\frac{(\\lambda x)^i}{i!}~~~~~\\text{(from Poisson distribution)}\\\\ \\Rightarrow f(x)&amp;=F&#39;(x)=\\lambda e^{-\\lambda x}\\left\\{1+\\sum_{i=1}^{\\alpha-1}\\left[\\frac{(\\lambda x)^i}{i!}-\\frac{(\\lambda x)^{i-1}}{(i-1)!}\\right]\\right\\}\\\\ &amp;=\\lambda^\\alpha e^{-\\lambda x}\\cdot\\frac{x^{\\alpha-1}}{(\\alpha-1)!} \\end{align} \\] Let \\(\\theta = \\frac{1}{\\lambda}\\), then \\[ f(x)=\\frac{1}{\\theta^\\alpha(\\alpha-1)!}e^{-x/\\theta}x^{\\alpha-1} \\] When \\(\\alpha=1\\), gamma distribution turns out to be exponential distribution. In the other hand, when \\(\\theta=2\\), \\(\\alpha=r/2\\), gamma distribution becomes Chi-square distribution. The Gamma distribution has a mean of \\(\\mu=\\alpha\\theta\\) and a variance of \\(\\sigma^2=\\alpha\\theta^2\\). The Gamma Function The gamma function, denoted \\(\\Gamma(t)\\), is defined, for \\(t\\geq 0\\), by: \\[ \\Gamma(t)=\\int_0^\\infty y^{t-1}e^{-y}dy \\] Moreover, \\[ \\Gamma(t)=\\frac{y^t}{t}\\cdot e^{-y}\\,\\bigg|_0^{\\infty}-\\int_0^\\infty\\frac{y^t}{t}\\cdot(-e^{-y})dy=\\frac{1}{t}\\Gamma(t+1) \\] therefore we have \\(\\Gamma(t+1)=t\\,\\Gamma(t)\\). In addition, when \\(t=1\\), \\[ \\Gamma(t)=\\int_0^\\infty e^{-y}dy=-e^{-y}\\,\\bigg|_0^{\\infty}=1 \\] In conclusion, for \\(n\\in\\mathbb{N}\\), \\[ \\Gamma(n)=(n-1)! \\] Chi-square Distribution Let \\(X\\) follow a gamma distribution with \\(\\theta=2\\) and \\(\\alpha=r/2\\), where r is a positive integer. Then the probability density function of X will be: \\[ f(x)=\\frac{1}{2^{r/2}\\Gamma(\\frac{r}{2})}e^{-x/2}x^{r/2-1}(x&gt;0) \\] Notice that the Gamma function is the analytic continuation of the factorial function. As \\(r/2\\) can be non-integer while \\(r\\) is an integer, we use the Gamma function here to replace the factorial function here. The expectation of \\(\\chi^2\\)-distribution is \\(\\mu=r\\), while the variance is \\(\\sigma^2=2r\\). \\(r\\) is called degree of freedom in \\(\\chi^2\\)-distribution. In addition, we have &gt; Theorem 1. If \\(Z_1,\\dots,Z_k\\sim\\mathcal{N}(0,1)\\) are independent, then the sum of their squares, &gt; \\[ &gt; Q\\ =\\sum _{i=1}^{k}Z_{i}^{2} &gt; \\] &gt; is distributed according to the chi-squared distribution with k degrees of freedom. &gt; &gt; Theorem 2. Let \\(X_i\\) denote \\(n\\) independent random variables that follow these chi-square distributions, e.g., \\(X_1\\sim\\chi^2(r_1)\\), \\(X_2\\sim\\chi^2(r_2)\\), etc. Then, the sum of the random variables &gt; \\[ &gt; Y=X_1+X_2+\\cdots+X_n &gt; \\] &gt; follows a chi-square distribution with \\(r_1+r_2+\\dots+r_n\\) degrees of freedom. That is: &gt; \\[ &gt; Y\\sim\\chi^2(r_1+r_2+\\dots+r_n) &gt; \\] While this can be proved with MGF, we're not going to introduce the proof here. Just take it as a conclusion. An interesting story of Beta/Dirichlet distribution See here. Normal Distribution (or Gaussian Distribution) The probability density function of \\(\\mathcal{N}(\\mu, \\sigma)\\) is \\[ {\\displaystyle f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}} \\] We can know from the last chapter (Basic knowledge in calculus) that \\[ \\int_{-\\infty}^{\\infty}{\\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}dx} = 1 \\] In addition, we have &gt; Theorem. Let \\(X_i\\) denote \\(n\\) independent random variables that follow these normal distributions. &gt; e.g., \\(X_1\\sim\\mathcal{N}(\\mu_1,\\sigma_1^2)\\), \\(X_2\\sim\\mathcal{N}(\\mu_2,\\sigma_2^2)\\), etc. Then, the linear combination &gt; \\[ &gt; Y=c_1X_1+c_2X_2+\\cdots+c_nX_n &gt; \\] &gt; follows the normal distribution: &gt; \\[ &gt; Y\\sim\\mathcal{N}(\\sum_{i=1}^nc_i\\mu_i,\\sum_{i=1}^nc_i^2\\sigma_i^2) &gt; \\] &gt; Z Scores It can be proved that, for \\(X\\sim \\mathcal{N}(0,1)\\), \\[ Z=\\frac{x-\\mu}{\\sigma}\\sim \\mathcal{N}(0,1) \\] This formula is also the defination of Z score. Relationship of Normal Distribution and \\(\\chi^2\\) Distribution Theorem. If \\(X\\) is normally distributed with mean \\(\\mu\\) and variance \\(\\sigma^2\\geq 0\\), then: \\[ V=\\left(\\frac{X-\\mu}{\\sigma}\\right)^2=Z^2 \\] is distributed as a chi-square random variable with 1 degree of freedom. Proof: Namely it's to prove that \\[ P(Z^2=v)=P(V=v)=g(v)=\\frac{1}{\\Gamma(1/2)2^{1/2}}e^{-v/2}v^{-1/2} \\] Meanwhile, \\[ \\begin{align} G(v)=P(Z^2\\leq v)&amp;=P(-\\sqrt{v}\\leq Z\\leq\\sqrt{v})\\\\ &amp;=\\int_{-\\sqrt v}^{\\sqrt v}\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}dx\\\\ &amp;=2\\int_0^{\\sqrt v}\\frac{1}{\\sqrt{2\\pi}}e^{-x^2/2}dx\\\\ &amp;=\\int_0^{v}\\frac{1}{\\sqrt{2\\pi}}z^{-1/2}e^{-z/2}dz\\\\ \\Rightarrow g(v)&amp;=\\frac{dG(v)}{dv}=\\frac{d\\left(\\int_0^{v}\\frac{1}{\\sqrt{2\\pi}}z^{-1/2}e^{-z/2}dz\\right)}{dv}\\\\ &amp;=\\frac{1}{\\sqrt{2\\pi}}v^{-1/2}e^{-v/2} \\end{align} \\] Also, \\[ \\begin{align} \\Gamma(1/2)&amp;=\\int_0^\\infty y^{-1/2}e^{-y}dy\\\\ &amp;=\\int_0^\\infty 2e^{-u^2}du\\\\ &amp;=\\int_{-\\infty}^\\infty e^{-u^2}du=\\sqrt{\\pi} \\end{align} \\] Therefore, \\[ g(v)=\\frac{1}{\\Gamma(1/2)2^{1/2}}e^{-v/2}v^{-1/2} \\] Our proof is complete. Moreover, we have \\[ \\frac{(n-1)S^2}{\\sigma^2}=\\frac{\\sum_{i=1}^{n}(X_i-\\bar{X})^2}{\\sigma^2}\\sim\\chi^2(n-1) \\] Central limit theorem Definition Let \\(X_1, X_2, \\dots, X_n\\) be a random sample from any distribution with (finite) mean \\(\\mu\\) and (finite) variance \\(\\sigma^2\\). If the sample size \\(n\\) is sufficiently large, then: the sample mean \\(\\bar{X}\\) follows an approximate normal distribution with mean \\(E(\\bar{X})=\\mu\\) and variance \\(Var(\\bar{X})=\\frac{\\sigma^2}{n}\\) Statistical Hypothesis Test About Null Hypothesis The null hypothesis is a general statement or default position that there is no relationship between two measured phenomena, or no association among groups. Testing (trying to accept or reject) the null hypothesis — and thus concluding that there is or is not a relationship between two phenomena— is a central task in the modern practice of science. Null Hypothesis is often denoted as \\(H_0\\), while the hypothesis being tested against it, also called the alternative hypothesis, is often denoted as \\(H_1\\). \\(p\\) is generally used for denoting the probability of \\(H_0\\), namely, the probability of there's no real difference. Student's t-Test The t-test is any statistical hypothesis test in which the test statistic follows a Student's t-distribution under the null hypothesis. It was introduced in 1908 by William Sealy Gosset under his pen name \"Student\". t-distribution Definition. If \\(Z\\sim\\mathcal{N}(0,1)\\) and \\(U\\sim\\chi^2(r)\\) are independent, then the random variable: \\[ T=\\frac{Z}{\\sqrt{U/r}} \\] follows a t-distribution with \\(r\\) degrees of freedom. We write \\(T\\sim t(r)\\). The p.d.f. of \\(T\\) is: \\[ \\frac{\\Gamma \\left(\\frac{\\nu+1}{2} \\right)} {\\sqrt{\\nu\\pi}\\,\\Gamma \\left(\\frac{\\nu}{2} \\right)} \\left(1+\\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}}\\! \\] It's clear that, for \\(X_1,X_2,\\dots,X_n\\sim\\mathcal{N}(\\mu,\\sigma^2)\\), \\[ \\frac{\\bar{X}-\\mu}{\\sigma/\\sqrt{n}}\\sim\\mathcal{N}(0,1) \\] And now we have: \\[ \\frac{\\bar{X}-\\mu}{s/\\sqrt{n}}\\sim t(n-1) \\] where \\(s\\) is the sample standard deviation. Most test statistics have the form \\(t=Z/S\\), where \\(Z\\) and \\(S\\) are functions of the data. \\(Z\\) may be sensitive to the alternative hypothesis (i.e., its magnitude tends to be larger when the alternative hypothesis is true) \\(S\\) is a scaling parameter that allows the distribution of t to be determined. \\(S^2\\) should follow a \\(\\chi^2\\) distribution with \\(p\\) degrees of freedom under the null hypothesis, where \\(p\\) is a positive constant One-sample t-test Let \\(Z=\\bar{X}-\\mu,\\,S=s/\\sqrt{n}\\), then \\[ {\\displaystyle t={\\frac {\\bar{x}-\\mu_{0}}{s/\\sqrt {n}}}} \\] Note that \\[ S^2=\\frac{s^2}{n}=\\sum_{i=1}^n\\frac{(X_i-\\bar{X})^2}{n(n-1)} \\] Two-sample t-test When to do this test \\(S\\) Degree of Freedom Small sample, \\(\\sigma_1^2=\\sigma_2^2\\) \\({\\displaystyle \\sqrt{s^2_{pool}\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)}}\\) \\(n_1+n_2-2\\) Small sample, \\(\\sigma_1^2\\neq\\sigma_2^2\\) \\({\\displaystyle \\sqrt{\\frac{s^2_1}{n_1}+\\frac{s^2_2}{n_2}}}\\) \\({\\displaystyle \\left.{\\left({\\frac {s_{1}^{2}}{n_{1}}}+{\\frac {s_{2}^{2}}{n_{2}}}\\right)^{2}}\\middle/\\left(\\frac {\\left(s_{1}^{2}~/~n_{1}\\right)^{2}}{n_{1}-1}+\\frac {\\left(s_{2}^{2}~/~n_{2}\\right)^{2}}{n_{2}-1}\\right)\\right.}\\) Z-Test A Z-test is any statistical test for which the distribution of the test statistic under the null hypothesis can be approximated by a normal distribution. Because of the central limit theorem, many test statistics are approximately normally distributed for large samples. Relationship with Student's t-Test For each significance level, the Z-test has a single critical value which makes it more convenient than the Student's t-test which has separate critical values for each sample size. Statistical tests with large sample size or known population variance can be conveniently performed as approximate Z-tests. If the population variance is unknown (and therefore has to be estimated from the sample itself) and the sample size is not large (n &lt; 30), the Student's t-test may be more appropriate. Pearson's chi-squared test Pearson's chi-squared test is used to assess three types of comparison: goodness of fit homogeneity independence It tests a null hypothesis stating that: The frequency distribution of certain events observed in a sample is consistent with a particular theoretical distribution. The events considered must be mutually exclusive and have total probability 1. Testing for statistical independence For a contingency table with \\(r\\) rows and \\(c\\) columns, the value of the test-statistic is \\[ \\chi ^{2}=\\sum_{i=1}^{r}\\sum_{j=1}^{c}{(O_{i,j}-E_{i,j})^{2}\\over E_{i,j}} \\] in which \\(O_{i,j},E_{i,j}\\) notes the observation and expectation in each cell, and the number of degrees of freedom \\(DF=(r-1)(c-1)\\). Yates' Correction for Continuity The approximation to the chi-squared distribution breaks down if expected frequencies are too low. Normally the approximation is acceptable when no more than 20% of the events have expected frequencies below 5. Where \\(DF=1\\), the approximation is acceptable when expected frequencies are no less than 10. In this case, a better approximation can be obtained by reducing the absolute value of each difference between observed and expected frequencies by 0.5 before squaring; this is called Yates's correction for continuity. ANOVA ANOVA, an abbreviation of Analysis of Variance, is a collection of statistical models and their associated estimation procedures used to analyze the differences among group means in a sample. In ANOVA, we generally make the following assumptions: Independence of observations – this is an assumption of the model that simplifies the statistical analysis. Normality – the distributions of the residuals are normal. Equality (or \"homogeneity\") of variances, called homoscedasticity — the variance of data in groups should be the same. One-way ANOVA Suppose we did a research about personal annual income in several major cities, and here's the survey result: Cities Samples Beijing \\(X_{11},X_{12},X_{13},\\dots\\) Shanghai \\(X_{21},X_{22},X_{23},\\dots\\) Guangzhou \\(X_{31},X_{32},X_{33},\\dots\\) Cities are called affect factor here. Now we want to know if there is a relationship between people's annual income and cities. For each group, assume the data obey a normal distribution \\(\\mathcal{N}(\\mu_i, \\sigma^2)\\), then the null hypothesis can be: \\[H_0:\\mu_1=\\cdots=\\mu_g\\] in which \\(g\\) is the number of groups. Here we consider three statistics, which is: The total sum of squares, namely the variance: \\[S_T^2=\\sum_{i=1}^{g}\\sum_{j=1}^{n_i}(X_{ij}-\\bar{X})^2\\] The intragroup sum of squares, introduced by affect factors: \\[S_A^2=\\sum_{i=1}^{g}n_i(\\bar{X_i}-\\bar{X})^2\\] The intergroup sum of squares, introduced by stochastic error: \\[S_E^2=\\sum_{i=1}^{g}\\sum_{j=1}^{n_i}(X_{ij}-\\bar{X_i})^2\\] We can easily prove that: \\[ S_T^2=S_A^2+S_E^2 \\] When \\(H_0\\) is true, we have: \\[ S_A\\sim\\chi^2(g-1),S_E\\sim\\chi^2(n-g) \\] Define \\(F\\) as the ratio of the between group variance and the within group variance, namely, \\[ F=\\frac{S_A/(g-1)}{S_E/(n-g)}\\sim F(g-1,n-g) \\] \\(F(g-1,n-g)\\) here means the F-distribution. Definition of F-distribution A random variate of the F-distribution with parameters d1 and d2 arises as the ratio of two scaled chi-squared variates: \\[ X=\\frac{U_1/d_1}{U_2/d_2} \\] where: U1 and U2 have chi-squared distributions with d1 and d2 degrees of freedom respectively, and U1 and U2 are independent. Therefore, under a given significance level \\(\\alpha\\), the rejection region is \\[ K_0={F&gt;F_{1-\\alpha}(g-1,n-g)} \\] Basic knowledge in calculus Some basic knowledge should be introduced ahead of normal distribution, as they can be helpful to understand the distribution function of normal distribution. ### Jacobi Matrix \\[\\mathbf {J} = {\\begin{bmatrix}{ \\dfrac{\\partial \\mathbf {f} }{\\partial x_{1}}} &amp; \\cdots &amp; {\\dfrac {\\partial \\mathbf {f} }{\\partial x_{n}}}\\end{bmatrix}}={\\begin{bmatrix} {\\dfrac {\\partial f_{1}}{\\partial x_{1}}} &amp; \\cdots &amp; {\\dfrac {\\partial f_{1}}{\\partial x_{n}}} \\\\ \\vdots &amp;\\ddots &amp;\\vdots \\\\ {\\dfrac {\\partial f_{m}}{\\partial x_{1}}} &amp; \\cdots &amp; {\\dfrac {\\partial f_{m}}{\\partial x_{n}}} \\end{bmatrix}}\\] Integration by substitution Let \\(x = \\varphi(u)\\), then we have \\[ \\begin{align} \\int_{\\varphi(a)}^{\\varphi(b)}{f(x)}\\,dx &amp;= \\int_{\\varphi(u)=\\varphi(a)}^{\\varphi(b)}{f(\\varphi(u))}\\,d\\varphi(u)\\\\ &amp;=\\int_a^b{f(\\varphi(u))\\varphi&#39;(u)\\,du} \\end{align} \\] Further, let \\((x, y) = (x(a, b), y(a, b))\\), then \\[ \\iint_{(x,y)\\in C} f(x, y)\\,dx\\,dy = \\iint_{(x(a,b),y(a,b))\\in C}{f(x(a,b), y(a,b))\\big|\\mathbf{J}\\big|\\,da\\,db} \\] in which \\(\\mathbf{J} = {\\begin{bmatrix} {\\dfrac {\\partial x}{\\partial a}}&amp; {\\dfrac {\\partial x}{\\partial b}}\\\\ {\\dfrac {\\partial y}{\\partial a}}&amp; {\\dfrac {\\partial y}{\\partial b}} \\end{bmatrix}}\\) is the Jacobi matrix in the instance. \\(\\big|\\mathbf{J}\\big|\\) means the determinant of the matrix. In particular, when replacing Cartesian coordinate system by polar coordinate system, we have \\[ \\begin{cases} x = rcos{\\theta}\\\\ y = rsin{\\theta} \\end{cases} \\] therefore, \\[ \\mathbf{J}=\\dfrac{\\partial (x,y)}{\\partial (r, \\theta)}={\\begin{bmatrix} cos{\\theta}&amp;-rsin{\\theta}\\&lt;span class=&quot;&quot;&gt;&lt;/span&gt;\\ sin{\\theta}&amp;rcos{\\theta} \\end{bmatrix}} \\Rightarrow|\\mathbf{J}| = r \\] Integration by parts If \\(u = u(x)\\) and \\(du = u&#39;(x)dx\\), while \\(v = v(x)\\) and \\(dv = v&#39;(x)dx\\), then integration by parts states that: \\[ \\begin{aligned} \\int _{a}^{b}u(x)v&#39;(x)\\,dx&amp;=[u(x)v(x)]_{a}^{b}-\\int _{a}^{b}u&#39;(x)v(x)dx\\\\ &amp;=u(b)v(b)-u(a)v(a)-\\int _{a}^{b}u&#39;(x)v(x)\\,dx \\end{aligned} \\] or, more compactly, \\[ \\int u\\,dv=uv-\\int v\\,du.\\! \\] Kronecker's delta \\[ \\delta_{i,j} = \\begin{cases} 1 &amp; \\text{if}~~i = j\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Intergrability of \\(x^ne^{-x^2/2}\\) Hermite Polynomial For each n, define the Hermite polynomial \\(H_n(x)\\) by \\[ \\frac{d^n}{dx^n}e^{-x^2/2}=(-1)^nH_n(x)e^{-x^2/2} \\] For example, \\[ H_0(x) = 1\\\\ \\frac{d}{dx}e^{-x^2/2}=-xe^{-x^2/2}\\Rightarrow H_1(x)=x\\\\ \\frac{d^2}{dx^2}e^{-x^2/2}=-e^{-x^2/2}+x^2e^{-x^2/2}\\Rightarrow H_2(x)=x^2-1\\\\ \\frac{d^3}{dx^3}e^{-x^2/2}=xe^{-x^2/2}+2xe^{-x^2/2}-x^3e^{-x^2/2}\\Rightarrow H_1(x)=x^3-3x \\] It's obvios that \\(\\int H_{n+1}{(x)}\\,e^{-x^2/2}=-H_{n}{(x)}\\,e^{-x^2/2}+C\\), assume that \\[ x^n=a_nH_n+a_{n-1}H_{n-1}+\\cdots+a_0H_0 \\] Because \\(H_k(x)e^{-x^2/2}\\) is integrable for \\(k \\geq 1\\), the integrability of \\(x^ne^{-x^2/2}\\) then depends on the value of \\(a_0\\). In linear algebra, \\({H_0, H_1, H_2, \\cdots}\\) form an orthogonal basis, and it has been proved that \\[ \\int_{-\\infty}^{\\infty} H_i(x)H_j(x)\\,e^{-x^2}dx=\\sqrt{2\\pi}\\,n!\\,\\delta(i, j) \\] Here \\[ a_0=\\frac{\\left&lt;x^n, H_0\\right&gt;}{\\left&lt;H_0, H_0\\right&gt;}=\\frac{1}{\\sqrt{2\\pi}}\\int_{-\\infty}^{\\infty}x^ne^{-x^2/2}dx~~\\begin{cases}&gt;0&amp;n~is~even\\\\ =0&amp;n~is~odd\\end{cases} \\] So we can say that If n is odd, \\(x^ne^{-x^2/2}\\) is intergrable Otherwise, if n is even, \\(x^ne^{-x^2/2}\\) is not intergrable Calculation of \\(\\int_{-\\infty}^{\\infty} e^{-x^2}dx\\) Though \\(H_0e^{-x^2}\\) is not intergrable, we can calculate the result of \\(\\int_{-\\infty}^{\\infty} e^{-x^2}dx\\). Suppose the result to be \\(I\\). then \\[ \\begin{split} I \\times I &amp;= \\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx} \\times \\int_{-\\infty}^{\\infty}{e^{-y^2}\\,dy} \\\\ &amp;= \\int_{y=-\\infty}^{\\infty}\\int_{x=-\\infty}^{\\infty}{e^{-(x^2+y^2)}\\,dx\\,dy}\\\\ &amp;= \\int_{\\theta=-\\pi}^{\\pi}\\int_{r=0}^{\\infty}re^{-r^2}\\,dr\\,d\\theta \\\\ &amp;= \\int_{\\theta=-\\pi}^{\\pi}\\left(-\\frac{1}{2}e^{-r^2}\\right)\\bigg|_{r=0}^{\\infty}\\,d\\theta \\\\ &amp;= \\int_{\\theta=-\\pi}^{\\pi}{-\\frac{1}{2}}\\,d\\theta \\\\ &amp;= \\pi \\end{split} \\] Apparently \\(I &gt; 0\\). Therefore, we have \\(I = \\sqrt{\\pi}\\) Calculation of \\(\\int_{-\\infty}^{\\infty}{x^{2k}e^{-x^2}dx}\\) Here we can use integration by parts. \\[ \\begin{align} \\int_{-\\infty}^{\\infty}{x^{2k}e^{-x^2}dx} &amp;= -\\frac{1}{2}\\int_{-\\infty}^{\\infty}{x^{2k-1}\\cdot\\left(-2xe^{-x^2}\\right)dx}\\\\ &amp;= -\\frac{1}{2}\\left(x^{2k-1}e^{-x^2}\\bigg|_{-\\infty}^{\\infty}-\\int_{-\\infty}^{\\infty}(2k-1)x^{2k-2}e^{-x^2}dx\\right)\\\\ &amp;= \\frac{2k-1}{2}\\int_{-\\infty}^{\\infty}{x^{2k-2}e^{-x^2}dx}\\\\ &amp;= \\frac{(2k-1)!!}{2^k}\\int_{-\\infty}^{\\infty} e^{-x^2}dx \\\\ &amp;= \\frac{(2k-1)!!}{2^k}\\sqrt{\\pi} \\end{align} \\] Moreover, \\[ {\\displaystyle \\int_{-\\infty}^{\\infty}{x^{2k}e^{-\\frac{(x-a)^2}{2b^2}}dx}=(2k-1)!!\\sqrt{2\\pi b^{2k}}} \\]","categories":[{"name":"科研 / Research","slug":"Research","permalink":"https://ayllenzhang.github.io/categories/Research/"}],"tags":[{"name":"Math","slug":"Math","permalink":"https://ayllenzhang.github.io/tags/Math/"},{"name":"Probablistic","slug":"Probablistic","permalink":"https://ayllenzhang.github.io/tags/Probablistic/"},{"name":"Note","slug":"Note","permalink":"https://ayllenzhang.github.io/tags/Note/"}]},{"title":"拯救 manjaro","slug":"save-manjaro","date":"2019-07-16T09:27:07.000Z","updated":"2019-07-24T11:24:53.000Z","comments":true,"path":"2019/07/save-manjaro/","link":"","permalink":"https://ayllenzhang.github.io/2019/07/save-manjaro/","excerpt":"在昨晚的一次 update 并重启以后，Manjaro 忽然黑屏，而我旁边只有一台手机。吓得本 Newbie 当场就在 Manjaro Forum 的 Newbie Corner 发了个帖子。然而发完回应者寥寥（就一个），所提的解决方法也无济于事，于是只好抱着小手机想办法。","text":"在昨晚的一次 update 并重启以后，Manjaro 忽然黑屏，而我旁边只有一台手机。吓得本 Newbie 当场就在 Manjaro Forum 的 Newbie Corner 发了个帖子。然而发完回应者寥寥（就一个），所提的解决方法也无济于事，于是只好抱着小手机想办法。 情况是这样的，在显示完系统选择界面，进入 Loading 环节时，屏幕上依次列出了四行信息 Loading Linux 5.0-rt-x86_64 ... Loading initial ramdisk ... mount: /sys/firmware/efi/efivars: unknown filesystem # 注：本报错此前就有 Starting version 242.32-3-arch 然后屏幕就定格在了黑屏，乌漆抹黑，只有左上角有一个凝固的下划线。我本来以为是更新后的初始化，结果吃了个饭回来发现还是这样，当时就惊了。更严重的是，甚至无法通过 Ctrl+Alt+Fx 进入 tty 。强制关机吧。按下关机键这个下划线还闪了两下，调皮。 玩笑归玩笑，事情还是要解决。第一个问题是如何进去。如果当时我有 MacBook Pro 在旁边的话有个很方便的解决办法，那就是 chroot ，具体实现方法是制作一个 Manjaro 启动盘，在启动盘系统加载好后把原硬盘中的分区mount到系统，再用chroot偷天换月，然后你就可以在这个临时系统里对原系统做调整了。 但是没有，怎么办呢？我首先参考了这个网页，通过在 grub 文件中 linux 为首的一行行末插入 init=/bin/bash 从而混入了系统。 但这还远远没完，进来之后发现系统仅挂在了 / 一个挂载点，同时网络也无法链接，测试证明网卡也未加载。在多次试验后，我写了这么一个脚本，在每次进系统后自动完成一系列配置操作。 #!/bin/sh # 挂载各分区 # lsblk -f # 显示分区情况 mount /dev/sdb3 /boot mount /dev/sdb2 /boot/efi mount /dev/sdb5 /home mount /dev/sda1 /home/Downloads # 配置网卡 # lspci -v | grep Ethernet -8 # 寻找网卡模块名称，Ethernet 下 Kernal Module 后面的就是了 modprobe e1000e # 这里我的 Module 是 e1000e ip link # 查看网卡名，其中不带 LOOPBACK 的那个是对的 ip link set eth0 up # 启用网卡 dhcpcd eth0 # 自动分配地址，如果失败要先 killall dhcpcd # ping www.baidu.com -c 5 # 这个界面不加 -c 居然 Ctrl+C 停不了，第一次配完又被迫重启了... 如此这般，看似简单，其实查了 N 个网页才查清楚上面一系列操作怎么完成。有人要说了，为什么要联网呢？因为是更新完出了错，首先要做的肯定是回滚更新。 cat /var/log/pacman.log | grep &quot;2019-07-15&quot; | grep -iE &#39;installed|upgraded|removed&#39; 这样就能具体看到今天 pacman 安装、升级、移除的包。通过 grep 的小技巧可以轻松回滚。 回滚完了以后系统依然无法进入，怀疑是更新导致了系统关键文件错误。由于在一开始的界面是可以更换 Linux 内核尝试启动的，而我尝试过后并没有效果，所以最初就排除了内核错误。而实验室的核显电脑自然也不会是因为显卡黑屏（尽管如此我还是尝试了重装驱动，无效）。所以最后能做的只有一点点翻看系统日志了。 journalctl -x -b -1 | grep -iE &#39;fail|error|unable&#39; # -x for more information, -b -1 for the last boot 然后翻到这么两行 Jul 15 22:53:25 manjaro /usr/lib/gdm-x-session[540]: /usr/lib/Xorg: error while loading shared libraries: libnettle.so.6: cannot open shared object file: No such file or directory Jul 15 22:53:25 manjaro /usr/lib/gdm-x-session[540]: Unable to run X server 这其实就很明显了。X server 是 Gnome 等一堆桌面环境的基础，没运行起来肯定进不了图形界面，于是搜索解决办法。解决方案非常简单，重装 nettle 包就行了。重装后执行 exec /sbin/init 轻松进入图形界面，收工。","categories":[{"name":"探索 / Explore","slug":"Explore","permalink":"https://ayllenzhang.github.io/categories/Explore/"}],"tags":[{"name":"Manjaro","slug":"Manjaro","permalink":"https://ayllenzhang.github.io/tags/Manjaro/"},{"name":"ArchLinux","slug":"ArchLinux","permalink":"https://ayllenzhang.github.io/tags/ArchLinux/"}]},{"title":"Manjaro 装机二三事","slug":"manjaro-install","date":"2019-06-24T05:31:08.000Z","updated":"2019-07-24T11:24:53.000Z","comments":true,"path":"2019/06/manjaro-install/","link":"","permalink":"https://ayllenzhang.github.io/2019/06/manjaro-install/","excerpt":"作为当前 DistroWatch 上排名第二的 Linux 发行版，基于 ArchLinux 的 Manjaro 在共享前者的 AUR 仓库，Wiki 及社区的同时，大大简化了 ArchLinux 的上手难度。他提供丰富的图形化界面选择，自动安装驱动，且拥有专用的软件仓库，是 Linux 初学者及进阶使用者的不二选择。 网上已经有很多的 Manjaro 装机报告了，我在此加以总结，并补充上我自己遇上的一些 bug 以及解决方案。","text":"作为当前 DistroWatch 上排名第二的 Linux 发行版，基于 ArchLinux 的 Manjaro 在共享前者的 AUR 仓库，Wiki 及社区的同时，大大简化了 ArchLinux 的上手难度。他提供丰富的图形化界面选择，自动安装驱动，且拥有专用的软件仓库，是 Linux 初学者及进阶使用者的不二选择。 网上已经有很多的 Manjaro 装机报告了，我在此加以总结，并补充上我自己遇上的一些 bug 以及解决方案。 Manjaro 的安装 镜像下载 下载地址：https://manjaro.org/download/ 选择 Official（官方发行）目录下或 Community（社区发行）目录下的发行版均可 推荐初学者使用 GNOME 或 Deepin（简单易用），进阶使用者使用 KDE （美观） 启动盘制作工具 macOS 推荐使用 Etcher Windows 推荐使用 Rufus 这两个都是一键配置，不像 UltraISO 有很多参数需要调。 安装过程 注意：双系统安装需要提前分区，请参考这个网页。 一个小 Trick 是：如果你有双硬盘，建议将 ~/Downloads 挂在机械硬盘上，然后利用 ln -s 建立软连接。例如，将 Documents 文件夹放在 ~/Downloads 下，但通过 ln -s 连接到主文件夹中。这样，Documents 就不会占用固态硬盘的空间，而我们仍可以通过 ~/Documents 直接访问该文件夹。 装机过程请参考这个网页。（很有缘，博客作者和我用的同一套 Hexo 主题，只是我调了颜色） Manjaro 系统配置 替换国内源 系统配置的第一步当然是配置软件源 在终端输入 sudo nano /etc/pacman.d/mirrorlist 在该文件顶端添加 Server = https://mirrors.tuna.tsinghua.edu.cn/archlinux/$repo/os/$arch 然后 Ctrl+O 写入，Ctrl+X 退出。 再次在终端输入 sudo nano /etc/pacman.conf 在文件末端加上 [archlinuxcn] Server = https://mirrors.tuna.tsinghua.edu.cn/archlinuxcn/$arch 同理保存退出 终端输入如下指令更新软件源： sudo pacman -Syy &amp;&amp; sudo pacman -S archlinuxcn-keyring 安装 yay 并配置 安装方式：终端输入如下代码 git clone https://aur.archlinux.org/yay.git cd yay makepkg -si 同样，将软件源修改为清华源提速： yay --aururl &quot;https://aur.tuna.tsinghua.edu.cn&quot; --save 安装输入法 此处采用fcitx + sogoupinyin 的安装方案 sogoupinyin实在是 bug 太多了，换用fcitx + cloudpinyin 的安装方案 sudo pacman -S fcitx-im fcitx-configtool sudo pacman -S fcitx-gtk2 fcitx-gtk3 fcitx-qt4 fcitx-qt5 # 都装全少点问题 sudo pacman -S fcitx-googlepinyin fcitx-cloudpinyin 编辑 ~/.xprofile 和 /etc/profile，加入（两个文件都要加入） export GTK_IM_MODULE=fcitx export QT_IM_MODULE=fcitx export XMODIFIERS=&quot;@im=fcitx&quot; 如果依然无法使用，在 ~/.profile 也加一份，并在 ~/.xinitrc 中加入 fcitx &amp;。 其他解决方法参考 here 安装 Git 并配置 ssh sudo pacman -S git git config --global user.name &lt;your_name&gt; git config --global user.email &lt;your_email&gt; cd ~/.ssh ssh-keygen -t rsa -C &quot;&lt;your_email&gt;&quot; # 然后回车两次，如果不需要 passphrase 的话 eval &quot;$(ssh-agent -s)&quot; # 后台启动 ssh-agent ssh-add ~/.ssh/id-rsa sudo pacman -S xclip xclip -sel clip &lt; ~/.ssh/id_rsa.pub # 此行指令将公钥复制到剪切板，粘贴到 Github 上添加即可 其他软件安装 安装 Chrome 及 node.js sudo pacman -S google-chrome nodejs npm 安装截图软件 deepin-screenshot sudo pacman -S deepin-screenshot 安装 VSCode sudo pacman -S visual-studio-code-bin 安装 QQ，TIM 和 WeChat sudo pacman -S deepin.com.qq.office sudo pacman -S deepin.com.qq.im sudo pacman -S electronic-wechat-git 安装 福昕阅读器，网易云，smplayer，goldendict yay -S foxitreader netease-cloud-music smplayer goldendict 注意 yay 不要加 sudo goldendict 需要手动添加词典，下载地址 （选装）Typora：一款轻便好用的 markdown 编辑器，支持所见即所得（我用这个写的博客） yay -s typora 安装字体 # 文泉驿 yay -S wqy-microhei-lite yay -S wqy-bitmapfont yay -S wqy-zenhei # Adobe 系列 yay -S adobe-source-han-sans-cn-fonts yay -S adobe-source-han-serif-cn-fonts yay -S noto-fonts-cjk 安装 SS / SSR SS 安装 sudo pacman -S shadowsocks-qt5 SSR 安装 # GFW 导致直接 yay -S electron-ssr 会失效 # 请直接到此处下载 pacman 包 https://github.com/qingshuisiyuan/electron-ssr-backup/releases # 如果 SSR 安装异常请先安装 gconf Tweaks 安装 Gnome Tweaks 是 Gnome 下的一款主题管理软件 sudo pacman -S gnome-tweak-tool 其他配置 配置快捷键 设置-设备-键盘-下划到最下方点击 + 号新建快捷键，自行设定即可，命令为要打开的软件名 文件夹默认打开方式出错解决方法（例如默认文件夹打开方式变为VSCode） 设置-应用-移除相应误开软件的文件打开方式绑定 配置主题后报错 Gtk-WARNING **: ... :Theme parsing error 在终端中输入 cd /usr/share/themes/ ls 找到你当前主题对应的文件夹（不清楚主题名称可在 Tweaks 中查看） 然后进入其下的 gtk-&lt;version&gt; 文件夹，&lt;version&gt; 对应你当前的GTK版本，大概率是其中的最新版。 然后根据报错信息手动修改文件夹中的 gtk.css 即可。","categories":[{"name":"探索 / Explore","slug":"Explore","permalink":"https://ayllenzhang.github.io/categories/Explore/"}],"tags":[{"name":"manjaro","slug":"manjaro","permalink":"https://ayllenzhang.github.io/tags/manjaro/"},{"name":"linux","slug":"linux","permalink":"https://ayllenzhang.github.io/tags/linux/"},{"name":"installation","slug":"installation","permalink":"https://ayllenzhang.github.io/tags/installation/"}]},{"title":"儿时的火车记忆","slug":"train-memory","date":"2019-05-31T18:25:14.000Z","updated":"2021-06-19T17:41:09.551Z","comments":true,"path":"2019/06/train-memory/","link":"","permalink":"https://ayllenzhang.github.io/2019/06/train-memory/","excerpt":"已经是2019年的儿童节了。十几年过去，童年时代的记忆早已被冲洗的很淡很淡，以至于我每次回想都有种在用恢复工具恢复误删照片的感觉——其中一些就是不见了，仿佛从未存在过，而另一些可能已经被其他文件痕迹所污染，辨不分明。但关于火车的记忆文件却从未消逝，反而在一次次访问时在脑内留下越来越深的痕迹。于是想着或许可以写些东西来纪念它们。","text":"已经是2019年的儿童节了。十几年过去，童年时代的记忆早已被冲洗的很淡很淡，以至于我每次回想都有种在用恢复工具恢复误删照片的感觉——其中一些就是不见了，仿佛从未存在过，而另一些可能已经被其他文件痕迹所污染，辨不分明。但关于火车的记忆文件却从未消逝，反而在一次次访问时在脑内留下越来越深的痕迹。于是想着或许可以写些东西来纪念它们。 十几年前，奥运到来之前，安检还仅仅存在于机场——至少在长沙这样的南方小城是这样。那时的入站颇有些大礼堂学生节入场的感觉，也许是因为都有着狭长的门厅与鹅黄色的灯光。走过漆痕斑驳的检票通道，就到了车站大厅。大堂正中央宽而慢的台阶连接着一楼与二楼，两层左右各有两个候车室，此刻倒又像是图书馆了。一楼候车室的尽头直连着一站台，那是长沙站最高贵的站台，毕竟在那个没有电梯的年代里全程不走楼梯进出站也算是种享受。通常，只有北京抵长的Z17有资格停靠，再不济也得是T20以内的车辆。另一个高贵的站台是4站台，作为曾经长沙站唯一的高站台，它通常是两列进京特快Z18/T2的出发站台。4站台是最西的站台，远端是远离城市喧嚣的树林与城郊风光。六点半发车的Z18往往会在落日与晚霞交相辉映之时拉响她悠长的汽笛，洗的透亮的银色的车顶映着霞光，向北缓缓驶去，高傲而美丽。 &gt; 上图为Z18使用的25T客车。一直认为这是国内最好看的客车车厢涂装，没有之一。可惜的是13年开始铁道部逐步开展了普速客车统型工作，现在早已是全国一片绿了。 坐的最多的要数去往湘西老家的红皮车。在湘西的山水隧桥间奔行的它，140的速度感觉上甚至比京广线上蓝皮的160还要快。除了春节，我爸有段时间负责老家的项目，每月往返于长沙与湘西，同样是乘坐那趟列车。我妈和我每次都会去买好站台票送他。3站台，九点发车，旁边停着的是另一辆橙皮——长沙至福州的1681次。两辆车在平日里都没有太多旅客，站台上很安静，旁边只有扶着手扶车小睡的小摊贩与偶尔路过的保洁员。大概是因为去的太多，火车司机偶尔会领我去驾驶室玩玩，让我拉响汽笛，然后送我下车，招手。画面像是老照片一样定格在那儿。 后来去了北京旅游，第一次坐了z18，当时的感受放现在应该类似于第一次坐头等舱飞纽约。第一次见到带真空集便器的干净厕所，车厢也很整洁，玻璃仿佛都别擦拭得亮一些，一切都很美好。开车前列车员会一遍遍提醒亲属及时下车，当年的直达还是真正的直达，没有任何中间停站，上错车就只能等到北京再下了。我还记得那天是满月，八年后我去学校报到还是同一趟车，还是满月。相同的还有这两次我都在循环着《当时的月亮》入睡，尽管用着的是不同的播放器，心态也大相径庭了。说起来北京站帮我解释了一个我小时候一直疑惑的问题：铁路线的尽头是什么样子？然后发现火车站就变成了火车栈（真实程序员笑话）。当然现在北京站和北京西修了地下联络线，已经不再是北京栈了。 beijing_north.png 图为此前的北京北站（栈），目前正在为迎接冬奥会而大改造中。 卧铺有着它独有的美好。作为从小睡眠困难的人，列车哐当哐当的响声是最好的催眠剂。与其说是车轮与铁轨敲响的简单节奏，不如说已经是是种令人心安的旋律了，想来可能和听着郭德纲和于谦的相声睡觉一个原理。想想那是车轮在铁轨连接处”跳跃”所发出的声音，好像还挺可爱的。然而除却呼噜声与体味之外，还有一不好之处。京广线上最漂亮的风景要数黄鹤楼与武汉长江大桥，铁路就修在黄鹤楼一墙之隔的地方，无论是从车上看游客还是从黄鹤楼里拍车都是很棒的体验。但如果坐卧铺回北京，就必然晚上经过这儿，从而看不到什么风景了。值得一提的事，作为一个老手，平日挑选车次不单看出发时间和运行时长，在白天经过风景路段同样会成为车次的加分项。 再后来，十四岁的时候，一个人去了甘肃旅行。兰州站挂着很多三角小彩旗，站台是很经典的蓝白配色。五年后我又去了一次，尽管只是路过，但意外的发现当年的小摊还在，旗子换成了方的但还是很好看。只是后面已然起了高楼，见不着山景了。说起小摊，又想起从桂林回来的路上，本来不饿，看到老太太很晚还在站台上卖东西就买了一盒方便面，泡完发现过期了... lanzhou.jpg 总之还有很多很多值得一谈的趣事，比如期待了几年才坐上最后却发现不过如此的双层火车，比如长沙到茶陵那趟连新开铺都停的五位数编号普快，比如在不坐火车的日子里经常会去看车的铁路桥以及桥下的岗亭，不是每趟车都会让守路员敬礼。但是也就随便闲聊，下次有缘再说吧。 去了那么多地方，也见了各种各样的风景，我却总是对那些夜里从火车上醒来的瞬间印象特别深。我记得夜色中皎洁月光照耀下的京广线，记得云中的朦胧月色洒在青藏高原上，记得沙丘远方天空将将泛起的鱼肚白，我想我是执着地爱着这种感觉的，或许是因为它与人生或多或少的相似之处。景色的精彩是人生的一部分，而大多数日子里我们却只是在被时间载着向不确定的远方行去。过去的日子如同一场大梦，醒来又不知身在何方，身边尽是寂静，想倾诉又不忍打扰他人的安宁，只听见车轮滚动如时间逝去的声音。 在这样的时候，能遇到恰好同时醒来的某人，相视一笑，然后轻轻聊些只有你们会聊的话题，是多么幸运的事。","categories":[{"name":"印象 / Impression","slug":"Impression","permalink":"https://ayllenzhang.github.io/categories/Impression/"}],"tags":[{"name":"Train","slug":"Train","permalink":"https://ayllenzhang.github.io/tags/Train/"}]}],"categories":[{"name":"远方 / Travel","slug":"Travel","permalink":"https://ayllenzhang.github.io/categories/Travel/"},{"name":"科研 / Research","slug":"Research","permalink":"https://ayllenzhang.github.io/categories/Research/"},{"name":"探索 / Explore","slug":"Explore","permalink":"https://ayllenzhang.github.io/categories/Explore/"},{"name":"印象 / Impression","slug":"Impression","permalink":"https://ayllenzhang.github.io/categories/Impression/"}],"tags":[{"name":"Japan","slug":"Japan","permalink":"https://ayllenzhang.github.io/tags/Japan/"},{"name":"Fuji","slug":"Fuji","permalink":"https://ayllenzhang.github.io/tags/Fuji/"},{"name":"Machine Learning","slug":"Machine-Learning","permalink":"https://ayllenzhang.github.io/tags/Machine-Learning/"},{"name":"notes","slug":"notes","permalink":"https://ayllenzhang.github.io/tags/notes/"},{"name":"MCMC","slug":"MCMC","permalink":"https://ayllenzhang.github.io/tags/MCMC/"},{"name":"Math","slug":"Math","permalink":"https://ayllenzhang.github.io/tags/Math/"},{"name":"Probablistic","slug":"Probablistic","permalink":"https://ayllenzhang.github.io/tags/Probablistic/"},{"name":"Note","slug":"Note","permalink":"https://ayllenzhang.github.io/tags/Note/"},{"name":"Manjaro","slug":"Manjaro","permalink":"https://ayllenzhang.github.io/tags/Manjaro/"},{"name":"ArchLinux","slug":"ArchLinux","permalink":"https://ayllenzhang.github.io/tags/ArchLinux/"},{"name":"manjaro","slug":"manjaro","permalink":"https://ayllenzhang.github.io/tags/manjaro/"},{"name":"linux","slug":"linux","permalink":"https://ayllenzhang.github.io/tags/linux/"},{"name":"installation","slug":"installation","permalink":"https://ayllenzhang.github.io/tags/installation/"},{"name":"Train","slug":"Train","permalink":"https://ayllenzhang.github.io/tags/Train/"}]}